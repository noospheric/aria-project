import re
import streamlit as st
import openai
from github import Github
from urllib.parse import urlparse
import pandas as pd

# --- App Config ---
st.set_page_config(
    page_title="Akifa.ai: EU AI Act Risk Analyzer",
    layout="wide",
    initial_sidebar_state="expanded",
)

official_legislation_url = "https://eur-lex.europa.eu/eli/reg/2024/1689"
# --- Branding & Logo ---
with st.sidebar:
    st.markdown("### EU AI Act Risk Analyzer")
    st.markdown("**Version:** [OJ_L_202401689](%s)" % official_legislation_url)

# --- Header & Metrics ---
st.title("ğŸ” Akifa.ai Enterprise")
col1, col2, col3, col4 = st.columns(4)
col1.metric("Repos Scanned", 7)
col2.metric("Data Sources Found", 11)
col3.metric("Identified Issues", 10)
col4.metric("Compliance Score", "85%", delta="+5%")

st.markdown("---")

# --- Input URL ---
github_url = st.text_input("Enter the GitHub repository URL:", placeholder="https://github.com/owner/repo")
if not github_url:
    st.info("ğŸ” Waiting for repository URL input...")
    st.stop()

# --- Fetch & Extract Metadata ---
with st.spinner("Scanning repository and extracting metadata..."):
    def extract_metadata(github_url: str) -> dict:
        token = st.secrets.get("GITHUB_TOKEN")
        gh = Github(token) if token else Github()
        path = urlparse(github_url).path.lstrip("/")
        owner, name = path.split("/")[:2]
        repo = gh.get_repo(f"{owner}/{name}")

        # README + requirements
        try:
            readme = repo.get_readme().decoded_content.decode()
        except:
            readme = ""
        try:
            req = repo.get_contents("requirements.txt")
            reqs = req.decoded_content.decode().splitlines()
        except:
            reqs = []

        # Core metadata
        languages    = repo.get_languages()
        topics       = repo.get_topics()
        license_id   = repo.get_license().license.spdx_id if repo.get_license() else "NONE"
        stars        = repo.stargazers_count
        forks        = repo.forks_count
        issues       = repo.open_issues_count
        last_push    = repo.pushed_at.isoformat()
        size_kb      = repo.size
        try:
            repo.get_contents(".github/workflows")
            has_ci = True
        except:
            has_ci = False
        contribs     = repo.get_contributors().totalCount

        blob = (readme + "\n" + "\n".join(reqs)).lower()
        tags = [kw for kw in ["finance","health","education","surveillance"] if kw in blob]

        # EU AI Act metrics
        biometric_data = "biometric" in blob
        human_in_loop  = "human-in-the-loop" in blob
        # Privacy Impact Assessment presence (Annex IV)
        try:
            file_list = repo.get_contents('.')
            pia_present = any(f.path.lower().endswith(('pia.md','privacy_impact_assessment.md')) for f in file_list)
        except:
            pia_present = False
        # Documentation coverage heuristic
        readme_summary = readme[:5000] + ("â€¦" if len(readme)>5000 else "")
        doc_coverage = "Good" if len(readme_summary) > 1000 else "Limited"

        return {
            "readme_summary": readme_summary,
            "requirements": reqs,
            "languages": languages,
            "topics": topics,
            "license": license_id,
            "stars": stars,
            "forks": forks,
            "open_issues": issues,
            "last_push": last_push,
            "size_kb": size_kb,
            "has_ci": has_ci,
            "contributors": contribs,
            "domain": tags[0] if tags else "General",
            "tags": tags,
            "biometric_data": biometric_data,
            "human_in_loop": human_in_loop,
            "pia_present": pia_present,
            "doc_coverage": doc_coverage
        }

    metadata = extract_metadata(github_url)

# --- Display Core Summary ---
st.header("ğŸ“Š Repository Analysis Summary")
with st.expander("Show Detailed Metadata", expanded=False):
    st.json(metadata)

# --- Charts & Graphs (Native) ---
st.subheader("Language Distribution (bytes %)")
lang_df = pd.DataFrame.from_dict(metadata['languages'], orient='index', columns=['bytes'])
lang_df['pct'] = lang_df['bytes'] / lang_df['bytes'].sum() * 100
st.bar_chart(lang_df['pct'])

# --- Domain & Topics Overview ---
st.subheader("Domain & Repository Topics")
st.metric("Domain Tag", metadata['domain'])
if metadata['topics']:
    st.write("**Topics:** " + ", ".join(metadata['topics']))
else:
    st.info("No GitHub topics identified.")

# --- Compliance Stats (Native) ---
st.subheader("ğŸ”’ EU AI Act & Governance Metrics")
col_a, col_b, col_c, col_d = st.columns(4)
col_a.metric("Biometric Data Used", "Yes" if metadata['biometric_data'] else "No")
col_b.metric("Human Oversight", "Yes" if metadata['human_in_loop'] else "No")
col_c.metric("PIA Document", "Found" if metadata['pia_present'] else "Not Found")
col_d.metric("Docs Coverage", metadata['doc_coverage'])

# Overall compliance checks
checks = [metadata['biometric_data'], not metadata['human_in_loop'], metadata['pia_present'], metadata['doc_coverage']=="Good"]
st.metric("Compliance Checks Passed", f"{sum(checks)}/{len(checks)}")

# Act-specific guidance
if metadata['biometric_data']:
    st.info("âš ï¸ Biometric processing is listed as a high-risk use under Annex II of the EU AI Act.")
if metadata['biometric_data'] and not metadata['human_in_loop']:
    st.warning("â— High-risk features detected without explicit human oversight â€” Article 14 requires adequate human-in-the-loop measures.")
if not metadata['pia_present']:
    st.warning("ğŸ“ Repository lacks a Privacy Impact Assessment (Annex IV requirement for high-risk AI systems).")

# Additional repository health stats
st.subheader("ğŸ“ˆ Repository Health Metrics")
h1, h2, h3, h4 = st.columns(4)
h1.metric("Stars", metadata['stars'])
h2.metric("Forks", metadata['forks'])
h3.metric("Open Issues", metadata['open_issues'])
h4.metric("Contributors", metadata['contributors'])

# --- LLM-powered Classification ---
st.subheader("ğŸ’¡ EU AI Act Risk Classification & Justification")
summary = (
    f"Summary (first 5000 chars): {metadata['readme_summary']}\n"
    f"Tags: {', '.join(metadata['tags']) or 'None'}\n"
    f"Domain: {metadata['domain']}\n"
    f"Requirements: {', '.join(metadata['requirements']) or 'None'}\n"
    f"Languages: {', '.join(f'{lang} ({pct:.0f}%)' for lang, pct in metadata['languages'].items())}\n"
    f"Topics: {', '.join(metadata['topics']) or 'None'}\n"
    f"License: {metadata['license']}\n"
    f"Stars: {metadata['stars']}, Forks: {metadata['forks']}, Issues: {metadata['open_issues']}\n"
    f"Last push: {metadata['last_push']}\n"
    f"Size (KB): {metadata['size_kb']}\n"
    f"CI configured: {'Yes' if metadata['has_ci'] else 'No'}\n"
    f"Contributors: {metadata['contributors']}\n"
    f"Biometric Data: {'Yes' if metadata['biometric_data'] else 'No'}\n"
    f"Human-in-the-loop: {'Yes' if metadata['human_in_loop'] else 'No'}"
)

openai.api_key = st.secrets["OPENAI_API_KEY"]
client = openai.OpenAI()
thread = client.beta.threads.create()
thread_id = thread.id
client.beta.threads.messages.create(
    thread_id=thread_id,
    role="user",
    content="Hereâ€™s the project summary for EU AI Act classification:\n\n" + summary
)
run = client.beta.threads.runs.create_and_poll(thread_id=thread_id, assistant_id="asst_DnkOcoj4OjCx5tu94QUp6X2L")
if run.status != "completed":
    st.error(f"Run status: {run.status}")
    st.stop()

steps_page = client.beta.threads.runs.steps.list(thread_id=thread_id, run_id=run.id)
tool_steps = [s for s in steps_page.data if getattr(s, 'type', None) == 'tool_calls']
file_search_step = next(s for s in tool_steps if any(hasattr(tc, 'file_search') for tc in s.step_details.tool_calls))
step_detail = client.beta.threads.runs.steps.retrieve(
    thread_id=thread_id, run_id=run.id, step_id=file_search_step.id,
    include=['step_details.tool_calls[*].file_search.results[*].content']
)

st.markdown("---")
st.markdown("### Risk Assessment & References")
page = client.beta.threads.messages.list(thread_id=thread_id)
assistant_msg = next(m for m in page.data if m.role == 'assistant')
st.write(assistant_msg.content[0].text.value)
shown = set()
for ann in assistant_msg.content[0].text.annotations:
    if ann.type == 'file_citation':
        idx = int(re.match(r"ã€\d+:(\d+)â€ sourceã€‘", ann.text).group(1))
        chunk = step_detail.step_details.tool_calls[0].file_search.results[idx].content[0].text
        if ann.text not in shown:
            st.markdown(f"**{ann.text}**")
            st.write(chunk)
            st.markdown('---')
            shown.add(ann.text)
